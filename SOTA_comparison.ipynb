{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d23fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497db0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qaSet_interpret = pd.read_csv('./Dataset/qa_set_interpret.csv', encoding='utf-8-sig')\n",
    "qaSet_engineer = pd.read_csv('./Dataset/qa_set.csv', encoding='utf-8-sig')\n",
    "# mefi_list = pd.read_csv('MeFi_result.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e631c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpreter_inst = \"You are designed to assist users in interpreting legal provisions related to the Korean Architecture Code. Your primary role is to provide interpretations of the laws as they apply to inquiries in Korean. The answer must consist of 3 parts: ‘질의요지’, ‘종합결론’, ‘판단근거’. The first part, ‘질의요지’, describes the core content of the inquiry. The second part, ‘종합결론’, describes the answer or conclusion of the question. The last part, ‘판단근거’, contains the legal basis of the answer described in ‘Comprehensive Conclusion’.\"\n",
    "\n",
    "# You may change the instruction to suit your needs. For example, you can use below instruction for BEQA: \n",
    "Extractive_inst = \"Return only the answer number first like Answer: [number], and then explain the reason.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed93d0d",
   "metadata": {},
   "source": [
    "## 1. Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec39ce6",
   "metadata": {},
   "source": [
    "### 1-1. Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb901c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have your own api key\n",
    "client = genai.Client(api_key=\"\")\n",
    "max_len = len(qaSet_interpret)\n",
    "print(max_len)\n",
    "writeFile = './result/AutoCon_Interpret_Gemini2.5f.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fd4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, max_len):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash-preview-04-17',\n",
    "        contents=[Interpreter_inst, qaSet_interpret['qeustion'][q_num]]\n",
    "    )\n",
    "    print(response.text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = str(response.usage_metadata) + \"\\nExecution Time: \"+str(end - start)\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = response.text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221c855",
   "metadata": {},
   "source": [
    "### 1-2. Extractive Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf1014",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = './result/AutoCon_PlainTest_Gemini2.5f.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, len(qaSet_engineer)):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-2.5-flash-preview-04-17',\n",
    "        contents=[Extractive_inst, qaSet_engineer['qeustion'][q_num]]\n",
    "    )\n",
    "    print(response.text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = str(response.usage_metadata) + \"\\nExecution Time: \"+str(end - start)\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = response.text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b64dc",
   "metadata": {},
   "source": [
    "## 2. GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2899940a",
   "metadata": {},
   "source": [
    "### 2-1. Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# You should have your own api key\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aa49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = len(qaSet_interpret)\n",
    "writeFile = './result/AutoCon_Interpret_GPT4.1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d016e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, max_len):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": Interpreter_inst\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": qaSet_interpret['qeustion'][q_num]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    output_text = response.choices[0].message.content\n",
    "    print(output_text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = str(response.usage) + \"\\nExecution Time: \"+str(end - start)\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = output_text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844eae9",
   "metadata": {},
   "source": [
    "### 2-2. Extractive Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = './result/AutoCon_PlainTest_GPT4.1.csv'\n",
    "max_len = len(qaSet_engineer)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c647a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, max_len):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": Extractive_inst\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": qaSet_engineer['qeustion'][q_num]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    output_text = response.choices[0].message.content\n",
    "    print(output_text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = str(response.usage) + \"\\nExecution Time: \"+str(end - start)\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = output_text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb9ab4",
   "metadata": {},
   "source": [
    "## 3. Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf370c",
   "metadata": {},
   "source": [
    "### 3-1. Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b7a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # You should have your own api key\n",
    "    api_key=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d198ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = './result/AutoCon_Interpret_Claude3.7_2.csv'\n",
    "max_len = len(qaSet_interpret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c66430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, max_len):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=2048,\n",
    "        system=Interpreter_inst,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": qaSet_interpret['qeustion'][q_num]}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response.content[0].text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = f\"Input Tokens: {response.usage.input_tokens}, Output Tokens: {response.usage.output_tokens},\\nExecution Time: {end - start} sec\"\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = response.content[0].text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456a34e8",
   "metadata": {},
   "source": [
    "### 3-2. Extractive Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = './result/AutoCon_PlainTest_Claude3.7.csv'\n",
    "max_len = len(qaSet_engineer)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80849b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, max_len):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        max_tokens=2048,\n",
    "        system=Extractive_inst,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": qaSet_engineer['qeustion'][q_num]}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(response.content[0].text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = f\"Input Tokens: {response.usage.input_tokens}, Output Tokens: {response.usage.output_tokens},\\nExecution Time: {end - start} sec\"\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = response.content[0].text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b06e48",
   "metadata": {},
   "source": [
    "## 4. Gemini1.5Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818d54c",
   "metadata": {},
   "source": [
    "### 4-1. Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2961f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have your own api key\n",
    "client = genai.Client(api_key=\"\")\n",
    "max_len = len(qaSet_interpret)\n",
    "print(max_len)\n",
    "writeFile = './result/AutoCon_Interpret_Gemini1.5Pro.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dae7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, max_len):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-1.5-pro-001',\n",
    "        contents=[Interpreter_inst, qaSet_interpret['qeustion'][q_num]]\n",
    "    )\n",
    "    print(response.text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = str(response.usage_metadata) + \"\\nExecution Time: \"+str(end - start)\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = response.text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7d92a",
   "metadata": {},
   "source": [
    "### 4-2. Extractive Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bac6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeFile = './result/AutoCon_PlainTest_Gemini1.5Pro.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c044ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_num in range(0, len(qaSet_engineer)):\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) Start-----\")\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model='gemini-1.5-pro-002',\n",
    "        contents=[Extractive_inst, qaSet_engineer['qeustion'][q_num]]\n",
    "    )\n",
    "    print(response.text)\n",
    "\n",
    "    end = time.time()\n",
    "    meta = str(response.usage_metadata) + \"\\nExecution Time: \"+str(end - start)\n",
    "\n",
    "    responseRecord = pd.read_csv(writeFile, dtype='string', encoding='utf-8-sig')\n",
    "    responseRecord.loc[q_num, 'metadata'] = meta\n",
    "    responseRecord.loc[q_num, 'response'] = response.text\n",
    "    responseRecord.to_csv(writeFile, encoding='utf-8-sig', index=False)\n",
    "\n",
    "    print(f\"Completed Responsing. Took {end - start:.5f} sec\")\n",
    "    print(f\"-----Question {q_num+1}(Index: {q_num}) End-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9e55f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
